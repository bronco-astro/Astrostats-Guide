<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Chapter 8 — Multivariate Analysis | Bronco's Astrostats Guide</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- MathJax for LaTeX equations -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']]
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    body {
      font-family: Arial, Helvetica, sans-serif;
      margin: 0;
      line-height: 1.6;
      background-color: #f9f9f9;
    }

    header {
      background-color: #222;
      color: white;
      padding: 2rem 1rem;
      text-align: center;
    }

    main {
      max-width: 800px;
      margin: auto;
      padding: 2rem 1rem;
    }

    h2 {
      margin-top: 2rem;
    }

    p {
      margin: 1rem 0;
    }

    a {
      text-decoration: none;
      color: #1a73e8;
    }

    a:hover {
      text-decoration: underline;
    }

    footer {
      text-align: center;
      padding: 1rem;
      background-color: #222;
      color: white;
      margin-top: 3rem;
      font-size: 0.9rem;
    }
  </style>
</head>

<body>

  <header>
    <h1>Bronco's Astrostats Guidance Website</h1>
    <p>Chapter 8 — Multivariate Analysis</p>
  </header>

  <main>

    <p>
      Multivariate analysis deals with data that contain more than one variable for
      each observation. Earlier topics often focused on one variable or pairs of
      variables. In this chapter, many variables are considered at the same time.
      We use PCA to group variable and test their influence.
    </p>

    <h2>Why multivariate methods are needed</h2>
    <p>
      Looking at one variable at a time can hide structure in the data.
      Some patterns only appear when several variables are considered together.
      Multivariate methods help describe these relationships in higher-dimensional spaces.
    </p>

    <h2>Multivariate data as points</h2>
    <p>
      Each observation can be viewed as a point in a space with many dimensions.
      Each dimension corresponds to one variable. A dataset becomes a cloud of points
      in this multi-dimensional space. Multivariate methods aim to summarize and
      understand the shape of this cloud.
    </p>

    <h2>Covariance between variables</h2>
    <p>
      Covariance measures how two variables vary together.
      If large values of one variable tend to occur with large values of another,
      the covariance is positive. If one variable tends to be large when the other
      is small, the covariance is negative.
    </p>

    <p>
      For two variables x and y, the sample covariance is
    </p>

    \[
      \mathrm{Cov}(x, y) =
      \frac{1}{n-1}
      \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
    \]

    <p>
      Covariance depends on the scale of the variables, which can make it difficult
      to compare across different units.
    </p>

    <h2>Correlation</h2>
    <p>
      Correlation is a scaled version of covariance.
      It removes units and restricts values to lie between −1 and 1, which makes
      interpretation easier.
    </p>

    <p>
      The sample correlation coefficient is
    </p>

    \[
      \mathrm{Corr}(x, y) =
      \frac{\mathrm{Cov}(x, y)}{\sigma_x \sigma_y}
    \]

    <p>
      where \(\sigma_x\) and \(\sigma_y\) are the standard deviations of x and y.
    </p>

    <h2>The covariance matrix</h2>
    <p>
      When working with many variables, covariances are collected into a covariance matrix.
      For p variables, the covariance matrix is a p × p matrix. Diagonal elements contain
      variances, and off-diagonal elements contain covariances between different variables.
    </p>

    <p>
      The covariance matrix summarizes relationships among all variables at once and plays
      a central role in multivariate analysis.
    </p>

    <h2>Principal component analysis (PCA)</h2>
    <p>
      Principal component analysis is a widely used multivariate technique.
      PCA finds new variables, called principal components, that are linear combinations
      of the original variables. These components are chosen to capture as much variance
      as possible.
    </p>

    <p>
      The first principal component captures the largest possible variance. Each following
      component captures the largest remaining variance under the constraint of being
      orthogonal to previous components.
    </p>

    <h2>PCA and eigenvalues</h2>
    <p>
      Mathematically, PCA is based on the covariance matrix.
      Principal components are obtained by solving the eigenvalue problem
    </p>

    \[
      \Sigma \mathbf{v} = \lambda \mathbf{v}
    \]

    <p>
      where \(\Sigma\) is the covariance matrix, \(\mathbf{v}\) is an eigenvector,
      and \(\lambda\) is the corresponding eigenvalue.
    </p>

    <p>
      Eigenvectors define the directions of the principal components. Eigenvalues measure
      how much variance is explained by each component.
    </p>

    <h2>Dimension reduction</h2>
    <p>
      High-dimensional data are difficult to visualize and analyze.
      PCA is often used to reduce the number of variables by keeping only the first few
      principal components. These components usually explain most of the variance in the data.
    </p>

    <h2>Scaling and standardization</h2>
    <p>
      Variables often have different units and ranges.
      Before applying PCA or covariance-based methods, data are often standardized so that
      each variable has mean zero and standard deviation one.
    </p>

    <p>
      Without scaling, variables with large numerical ranges can dominate the results.
    </p>

    <h2>Multivariate normal distribution</h2>
    <p>
      Some multivariate methods assume data follow a multivariate normal distribution.
      This distribution is described by a mean vector and a covariance matrix.
      It generalizes the one-dimensional normal distribution to higher dimensions.
    </p>

    <h2>Limits of multivariate methods</h2>
    <p>
      Multivariate techniques are powerful but sensitive to noise, outliers, and preprocessing choices.
      Interpretation becomes harder as the number of variables increases. Understanding assumptions
      is important before drawing conclusions.
    </p>

    <p>
      <a href="index.html">← Back to Home</a>
    </p>

  </main>

  <footer>
    <p>© <span id="year"></span> Astrostats Guide</p>
  </footer>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>

</body>
</html>
